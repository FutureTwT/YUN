{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os  \n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import re\n",
    "\n",
    "def get_data():\n",
    "    train = pd.read_csv(r'C:\\Data\\train_first.csv')\n",
    "    test = pd.read_csv(r'C:\\Data\\predict_first.csv')\n",
    "    data = pd.concat([train, test])\n",
    "    print('train %s test %s'%(train.shape,test.shape))\n",
    "    print('train columns',train.columns)\n",
    "    return data,train.shape[0],train['Score'],test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_str(string):\n",
    "    string = re.sub(r'[0-9a-zA-Z]+', '', string)\n",
    "    string = string.replace('，','').replace('。','').replace('～','').replace(' ','').replace('！','').\\\n",
    "        replace('<br/>','').replace('；','').replace('）','').replace('（','').replace('.','').\\\n",
    "        replace('“','').replace('”','').replace(',','').replace('【','').replace('】','').replace('~','').\\\n",
    "        replace('\\xa0','').replace('《','').replace('》','').replace('<','').replace('>','').replace('/','').strip()\n",
    "    cut_str = jieba.cut(string.strip())\n",
    "    list_str = [word for word in cut_str]\n",
    "    string = ' '.join(list_str)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_discuss(data):\n",
    "    data['length'] = data['Discuss'].apply(lambda x:len(x))\n",
    "    data['Discuss'] = data['Discuss'].apply(lambda x: clear_str(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process():\n",
    "    data,nrw_train,y,test_id = get_data()\n",
    "    data = split_discuss(data)\n",
    "    cv = CountVectorizer(ngram_range=(1,2))\n",
    "    discuss = cv.fit_transform(data['Discuss'])\n",
    "    tf = TfidfVectorizer(max_df=10000,ngram_range=(1,2))\n",
    "    discuss_tf = tf.fit_transform(data['Discuss'])\n",
    "    # length = csr_matrix(pd.get_dummies(data['length'],sparse=True).values)\n",
    "    data = hstack((discuss,discuss_tf)).tocsr()\n",
    "    return data[:nrw_train],data[nrw_train:],y,test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xx_mse_s(y_true,y_pre):\n",
    "    y_true = y_true\n",
    "    y_pre = pd.DataFrame({'res':list(y_pre)})\n",
    "\n",
    "    y_pre['res'] = y_pre['res'].astype(int)\n",
    "    return 1 / ( 1 + mean_squared_error(y_true,y_pre['res'].values)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (100000, 3) test (30000, 2)\n('train columns', Index([u'Id', u'Discuss', u'Score'], dtype='object'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Trie..., from C:\\Software\\Anaconda2\\lib\\site-packages\\jieba\\dict.txt\nloading model from cache c:\\users\\miconron\\appdata\\local\\temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dumping model to file cache c:\\users\\miconron\\appdata\\local\\temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading model cost  2.69099998474 seconds.\nTrie has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "X, test, y, test_id = pre_process()\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "kf = kf.split(X)\n",
    "# get model\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "import lightgbm\n",
    "\n",
    "cv_pred = []\n",
    "xx_mse = []\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "SVM = SVC()\n",
    "Ri = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=250, normalize=False, tol=0.01)\n",
    "RF = RandomForestClassifier(random_state=10)\n",
    "EXT = ExtraTreesClassifier()\n",
    "LR = LogisticRegression()\n",
    "\n",
    "SCLF = StackingCVClassifier(classifiers=[DT, SVM, Ri, RF, EXT, LR], meta_classifier=lr, use_probas=True, n_folds=3, verbose=3)\n",
    "for i ,(train_fold,test_fold) in enumerate(kf):\n",
    "    X_train, X_validate, label_train, label_validate = X[train_fold, :], X[test_fold, :], y[train_fold], y[test_fold]\n",
    "    SCLF.fit(X_train, label_train)\n",
    "    val_ = SCLF.predict(X_validate)\n",
    "    print(xx_mse_s(label_validate, val_))\n",
    "    cv_pred.append(model_1.predict(test))\n",
    "    xx_mse.append(xx_mse_s(label_validate, val_))\n",
    "    \n",
    "import numpy as np\n",
    "print('xx_result',np.mean(xx_mse))\n",
    "\n",
    "s = 0\n",
    "for i in cv_pred:\n",
    "    s = s + i\n",
    "\n",
    "s = s/3\n",
    "res = pd.DataFrame()\n",
    "res['Id'] = list(test_id)\n",
    "res['pre'] = list(s)\n",
    "\n",
    "res.to_csv(r'C:\\Data\\t_20180215_1.csv',index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
